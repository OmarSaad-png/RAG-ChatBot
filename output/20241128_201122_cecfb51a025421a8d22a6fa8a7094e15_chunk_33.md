toring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[67]After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to bei.i.d, standard statistical tools can be used for hypothesis testing, such asT-testandpermutation test.[68]This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[69]See also[edit]Temporal difference learningQ-learningStateactionrewardstateaction(SARSA)Reinforcement learning from human feedbackOptimal controlError-driven learningMulti-agent reinforcement learningApprenticeship learningModel-free (reinforcement learning)active learning (machine learning)References[edit]^Kaelbling, Leslie P.;Littman, Michael L.;Moore, Andrew W.(1996)."Reinforcement Learning: A Survey".Journal of Artificial Intelligence Research.4: 237285.arXiv:cs/9605103.doi:10.1613/jair.301.S2CID1708582. Archived fromthe originalon 2001-11-20.^van Otterlo, M.; Wiering, M. (2012). "Reinforcement Learning and Markov Decision Processes".Reinforcement Learning. Adaptation, Learning, and Optimization. Vol. 12. pp. 342.doi:10.1007/978-3-642-27645-3_1.ISBN978-3-642-27644-6.^abLi, Shengbo (2023).Reinforcement Learning for Sequential Decision and Optimal Control(First ed.). Springer Verlag, Singapore. pp. 1460.doi:10.1007/978-981-19-7784-8.ISBN978-9-811-97783-1.S2CID257928563.{{cite book}}: CS1 maint: location missing publisher (link)^Russell, Stuart J.; Norvig, Peter (2010).Artificial intelligence : a modern approach(Third ed.). Upper Saddle River, New Jersey. pp. 830, 831.ISBN978-0-13-604259-4.{{cite book}}: CS1 maint: location missing publisher (link)^Lee, Daeyeol; Seo, Hyojung; Jung, Min Whan (21 July 2012)."Neural Basis of Reinforcement Learning and Decision Making".Annual Review of Neuroscience.35(1): 287308.doi:10.1146/annurev-neuro-062111-150512.PMC3490621.PMID22462543.^Salazar Duque, Edgar Mauricio; Giraldo, Juan S.; Vergara, Pedro P.; Nguyen, Phuong; Van Der Molen, Anne; Slootweg, Han (2022)."Community energy storage operation via reinforcement learning with eligibility traces".Electric Power Systems Research.212.Bibcode:2022EPSR..21208515S.doi:10.1016/j.epsr.2022.108515.S2CID250635151.^Xie, Zhaoming; Hung Yu Ling; Nam Hee Kim; Michiel van de Panne (2020). "ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills".arXiv:2005.04323[cs.GR].^Vergara, Pedro P.; Salazar, Mauricio; Giraldo, Juan S.; Palensky, Peter (2022)."Optimal dispatch of PV inverters in unbalanced distribution systems using Reinforcement Learning".International Journal of Electrical Power & Energy Systems.136.Bibcode:2022IJEPE.13607628V.doi:10.1016/j.ijepes.2021.107628.S2CID244099841.^Sutton & Barto 2018, Chapter 11.^Ren, Yangang; Jiang, Jianhua; Zhan, Guojian; Li, Shengbo Eben; Chen, Chen; Li, Keqiang; Duan, Jingliang (2022)."Self-Learned Intelligence for Integrated Decision and Control of Automated Vehicles at Signalized Intersections".IEEE Transactions on Intelligent Transportation Systems.23(12): 2414524156.arXiv:2110.12359.doi:10.1109/TITS.2022.3196167.^Gosavi, Abhijit(2003).Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement. Operations Research/Computer Science Interfaces Series. Springer.ISBN978-1-4020-7454-7.^abBurnetas, Apostolos N.;Katehakis, Michael N.(1997), "Optimal adaptive policies for Markov Decision Processes",Mathematics of Operations Research,22(1): 222255,doi:10.1287/moor.22.1.222,JSTOR3690147^Tokic, Michel; Palm, Gnther (2011),"Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax"(PDF),KI 2011: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 7006, Springer, pp. 335346,ISBN978-3-642-24455-1^abc"Reinforcement learning: An introduction"(PDF). Archived fromthe original(PDF)on 2017-07-12. Retrieved2017-07-23.^Singh, Satinder P.; Sutton, Richard S. (1996-03-01)."Reinforcement learning with replacing eligibility traces".Machine Learning.22(1): 123158.doi:10.1007/BF00114726.ISSN1573-0565.^Sutton, Richard S.(1984).Temporal Credit Assignment in Reinforcement Learning(PhD thesis). University of Massachusetts, Amherst, MA. Archived fromthe originalon 2017-03-30. Retrieved2017-03-29.^Sutton & Barto 2018,6. Temporal-Difference Learning.^Bradtke, Steven J.;Barto, Andrew G.(1996). "Learning to predict by the method of temporal differences".Machine Learning.22: 3357.CiteSeerX10.1.1.143.857.doi:10.1023/A:10