18056104778.S2CID20327856.^Watkins, Christopher J.C.H.(1989).Learning from Delayed Rewards(PDF)(PhD thesis). Kings College, Cambridge, UK.^Matzliach, Barouch; Ben-Gal, Irad; Kagan, Evgeny (2022)."Detection of Static and Mobile Targets by an Autonomous Agent with Deep Q-Learning Abilities".Entropy.24(8): 1168.Bibcode:2022Entrp..24.1168M.doi:10.3390/e24081168.PMC9407070.PMID36010832.^Williams, Ronald J.(1987). "A class of gradient-estimating algorithms for reinforcement learning in neural networks".Proceedings of the IEEE First International Conference on Neural Networks.CiteSeerX10.1.1.129.8871.^Peters, Jan;Vijayakumar, Sethu;Schaal, Stefan(2003).Reinforcement Learning for Humanoid Robotics(PDF). IEEE-RAS International Conference on Humanoid Robots. Archived fromthe original(PDF)on 2013-05-12.^Juliani, Arthur (2016-12-17)."Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)".Medium. Retrieved2018-02-22.^Deisenroth, Marc Peter;Neumann, Gerhard;Peters, Jan(2013).A Survey on Policy Search for Robotics(PDF). Foundations and Trends in Robotics. Vol. 2. NOW Publishers. pp. 1142.doi:10.1561/2300000021.hdl:10044/1/12051.^Sutton, Richard (1990). "Integrated Architectures for Learning, Planning and Reacting based on Dynamic Programming".Machine Learning: Proceedings of the Seventh International Workshop.^Lin, Long-Ji (1992)."Self-improving reactive agents based on reinforcement learning, planning and teaching"(PDF).Machine Learning volume 8.doi:10.1007/BF00992699.^Zou, Lan (2023-01-01), Zou, Lan (ed.),"Chapter 7 - Meta-reinforcement learning",Meta-Learning, Academic Press, pp. 267297,doi:10.1016/b978-0-323-89931-4.00011-0,ISBN978-0-323-89931-4, retrieved2023-11-08^van Hasselt, Hado; Hessel, Matteo; Aslanides, John (2019)."When to use parametric models in reinforcement learning?"(PDF).Advances in Neural Information Processing Systems 32.^Grondman, Ivo; Vaandrager, Maarten; Busoniu, Lucian; Babuska, Robert; Schuitema, Erik (2012-06-01)."Efficient Model Learning Methods for ActorCritic Control".IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics).42(3): 591602.doi:10.1109/TSMCB.2011.2170565.ISSN1083-4419.PMID22156998.^"On the Use of Reinforcement Learning for Testing Game Mechanics : ACM - Computers in Entertainment".cie.acm.org. Retrieved2018-11-27.^Riveret, Regis; Gao, Yang (2019). "A probabilistic argumentation framework for reinforcement learning agents".Autonomous Agents and Multi-Agent Systems.33(12): 216274.doi:10.1007/s10458-019-09404-2.S2CID71147890.^Yamagata, Taku; McConville, Ryan; Santos-Rodriguez, Raul (2021-11-16). "Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills".arXiv:2111.08596[cs.LG].^Kulkarni, Tejas D.; Narasimhan, Karthik R.; Saeedi, Ardavan; Tenenbaum, Joshua B. (2016)."Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation".Proceedings of the 30th International Conference on Neural Information Processing Systems. NIPS'16. USA: Curran Associates Inc.: 36823690.arXiv:1604.06057.Bibcode:2016arXiv160406057K.ISBN978-1-5108-3881-9.^"Reinforcement Learning / Successes of Reinforcement Learning".umichrl.pbworks.com. Retrieved2017-08-06.^Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (March 2020)."User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs".2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)(PDF). pp. 17281733.doi:10.23919/DATE48585.2020.9116294.ISBN978-3-9819263-4-7.S2CID219858480.^Quested, Tony."Smartphones get smarter with Essex innovation".Business Weekly. Retrieved2021-06-17.^Williams, Rhiannon (2020-07-21)."Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'".i. Retrieved2021-06-17.^Kaplan, F.; Oudeyer, P. (2004). "Maximizing Learning Progress: An Internal Reward System for Development". In Iida, F.; Pfeifer, R.; Steels, L.; Kuniyoshi, Y. (eds.).Embodied Artificial Intelligence. Lecture Notes in Computer Science. Vol. 3139. Berlin; Heidelberg: Springer. pp. 259270.doi:10.1007/978-3-540-27833-7_19.ISBN978-3-540-22484-6.S2CID9781221.^Klyubin, A.; Polani, D.; Nehaniv, C. (2008)."Keep your options open: an information-based driving principle for sensorimotor systems".PLOS ONE.3(12): e4018.Bibcode:2008PLoSO...3.4018K.doi:10.1371/journal.pone.0004018.PMC2607028.PMID19107219.^Barto, A. G. (2013). "Intrinsic motivation and reinforcement learning".Intrinsically Motivated Learning in Natural and Artificial Systems(PDF). Berlin; Heidelberg: Springer. pp. 1747.^Dabrius, Kevin; Granat, Elvin; Karlsson, Patrik (2020). "Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks".The Journal of Machine Learning in Finance.1.SSRN3374766.^George Karimpanal, Thommen; Bouffanais, Roland (2019). "Self-organizing maps for storage and transfer of knowledge in reinforcement learning".Adaptive Behavior.27(2): 