l(PDF)on 2013-05-12.^Juliani, Arthur (2016-12-17)."Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)".Medium. Retrieved2018-02-22.^Deisenroth, Marc Peter;Neumann, Gerhard;Peters, Jan(2013).A Survey on Policy Search for Robotics(PDF). Foundations and Trends in Robotics. Vol. 2. NOW Publishers. pp. 1142.doi:10.1561/2300000021.hdl:10044/1/12051.^Sutton, Richard (1990). "Integrated Architectures for Learning, Planning and Reacting based on Dynamic Programming".Machine Learning: Proceedings of the Seventh International Workshop.^Lin, Long-Ji (1992)."Self-improving reactive agents based on reinforcement learning, planning and teaching"(PDF).Machine Learning volume 8.doi:10.1007/BF00992699.^Zou, Lan (2023-01-01), Zou, Lan (ed.),"Chapter 7 - Meta-reinforcement learning",Meta-Learning, Academic Press, pp. 267297,doi:10.1016/b978-0-323-89931-4.00011-0,ISBN978-0-323-89931-4, retrieved2023-11-08^van Hasselt, Hado; Hessel, Matteo; Aslanides, John (2019)."When to use parametric models in reinforcement learning?"(PDF).Advances in Neural Information Processing Systems 32.^Grondman, Ivo; Vaandrager, Maarten; Busoniu, Lucian; Babuska, Robert; Schuitema, Erik (2012-06-01)."Efficient Model Learning Methods for ActorCritic Control".IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics).42(3): 591602.doi:10.1109/TSMCB.2011.2170565.ISSN1083-4419.PMID22156998.^"On the Use of Reinforcement Learning for Testing Game Mechanics : ACM - Computers in Entertainment".cie.acm.org. Retrieved2018-11-27.^Riveret, Regis; Gao, Yang (2019). "A probabilistic argumentation framework for reinforcement learning agents".Autonomous Agents and Multi-Agent Systems.33(12): 216274.doi:10.1007/s10458-019-09404-2.S2CID71147890.^Yamagata, Taku; McConville, Ryan; Santos-Rodriguez, Raul (2021-11-16). "Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills".arXiv:2111.08596[cs.LG].^Kulkarni, Tejas D.; Narasimhan, Karthik R.; Saeedi, Ardavan; Tenenbaum, Joshua B. (2016)."Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation".Proceedings of the 30th International Conference on Neural Information Processing Systems. NIPS'16. USA: Curran Associates Inc.: 36823690.arXiv:1604.06057.Bibcode:2016arXiv160406057K.ISBN978-1-5108-3881-9.^"Reinforcement Learning / Successes of Reinforcement Learning".umichrl.pbworks.com. Retrieved2017-08-06.^Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (March 2020)."User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs".2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)(PDF). pp. 17281733.doi:10.23919/DATE48585.2020.9116294.ISBN978-3-9819263-4-7.S2CID219858480.^Quested, Tony."Smartphones get smarter with Essex innovation".Business Weekly. Retrieved2021-06-17.^Williams, Rhiannon (2020-07-21)."Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'".i. Retrieved2021-06-17.^Kaplan, F.; Oudeyer, P. (2004). "Maximizing Learning Progress: An Internal Reward System for Development". In Iida, F.; Pfeifer, R.; Steels, L.; Kuniyoshi, Y. (eds.).Embodied Artificial Intelligence. Lecture Notes in Computer Science. Vol. 3139. Berlin; Heidelberg: Springer. pp. 259270.doi:10.1007/978-3-540-27833-7_19.ISBN978-3-540-22484-6.S2CID9781221.^Klyubin, A.; Polani, D.; Nehaniv, C. (2008)."Keep your options open: an information-based driving principle for sensorimotor systems".PLOS ONE.3(12): e4018.Bibcode:2008PLoSO...3.4018K.doi:10.1371/journal.pone.0004018.PMC2607028.PMID19107219.^Barto, A. G. (2013). "Intrinsic motivation and reinforcement learning".Intrinsically Motivated Learning in Natural and Artificial Systems(PDF). Berlin; Heidelberg: Springer. pp. 1747.^Dabrius, Kevin; Granat, Elvin; Karlsson, Patrik (2020). "Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks".The Journal of Machine Learning in Finance.1.SSRN3374766.^George Karimpanal, Thommen; Bouffanais, Roland (2019). "Self-organizing maps for storage and transfer of knowledge in reinforcement learning".Adaptive Behavior.27(2): 111126.arXiv:1811.08318.doi:10.1177/1059712318818568.ISSN1059-7123.S2CID53774629.^J Duan; Y Guan; S Li (2021)."Distributional Soft Actor-Critic: Off-policy reinforcement learning for addressing value estimation errors".IEEE Transactions on Neural Networks and Learning Systems.33(11): 65846598.arXiv:2001.02811.doi:10.1109/TNNLS.2021.3082568.PMID34101599.S2CID211259373.^Y Ren; J Duan; S Li (2020)."Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic".2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). pp. 16.arXiv:2002.05502.doi:10.1109/ITSC45102.2020.9294300.ISBN978-1-7281-4149-7.S2CID211096594.^Duan, J; Wang, W; Xiao, L (2023-10-26). "DSAC-T: Distributional Soft Actor-Critic with Three Refinement