states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).[1]The search for this balance is known as theexploration-exploitation dilemma. The environment is typically stated in the form of aMarkov decision process(MDP), as many reinforcement learning algorithms usedynamic programmingtechniques.[2]The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.[3] Introduction[edit] Due to its generality, reinforcement learning is studied in many disciplines, such asgame theory,control theory,operations research,information theory,simulation-based optimization,multi-agent systems,swarm intelligence, andstatistics. In the operations research and control literature, RL is calledapproximate dynamic programming, orneuro-dynamic programming.The problems of interest in RL have also been studied in thetheory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment). Basic reinforcement learning is modeled as aMarkov decision process: The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar toprocessesthat appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5] A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time stept, the agent receives the current stateSt{\displaystyle S_{t}}and rewardRt{\displaystyle R_{t}}. It then chooses an actionAt{\displaystyle A_{t}}from the set of available actions, which is subsequently sent to the environment. The environment moves to a new stateSt+1{\displaystyle S_{t+1}}and the rewardRt+1{\displaystyle R_{t+1}}associated with thetransition(St,At,St+1){\displaystyle (S_{t},A_{t},S_{t+1})}is determined. The goal of a reinforcement learning agent is to learn apolicy: :SA[0,1]{\displaystyle \pi :{\mathcal {S}}\times {\mathcal {A}}\rightarrow [0,1]},(s,a)=Pr(At=aSt=s){\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)} that maximizes the expected cumulative reward. Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to havefull observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to havepartial observability, and formally the problem must be formulated as apartially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed. When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion ofregret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative. Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, includingenergy storage,[6]robot control,[7]photovoltaic generators,[8]backgammon,checkers,[9]Go(AlphaGo), andautonomous driving systems.[10] Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use offunction approximationto deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations: The first two