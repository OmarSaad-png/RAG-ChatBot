s".arXiv:2310.05858[cs.LG].^Soucek, Branko (6 May 1992).Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series. John Wiley & Sons, Inc. p. 38.ISBN0-471-55717-X.^Francois-Lavet, Vincent; et al. (2018). "An Introduction to Deep Reinforcement Learning".Foundations and Trends in Machine Learning.11(34): 219354.arXiv:1811.12560.Bibcode:2018arXiv181112560F.doi:10.1561/2200000071.S2CID54434537.^Mnih, Volodymyr; et al. (2015). "Human-level control through deep reinforcement learning".Nature.518(7540): 529533.Bibcode:2015Natur.518..529M.doi:10.1038/nature14236.PMID25719670.S2CID205242740.^Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). "Explaining and Harnessing Adversarial Examples".International Conference on Learning Representations.arXiv:1412.6572.^Behzadan, Vahid; Munir, Arslan (2017). "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks".Machine Learning and Data Mining in Pattern Recognition. Lecture Notes in Computer Science. Vol. 10358. pp. 262275.arXiv:1701.04143.doi:10.1007/978-3-319-62416-7_19.ISBN978-3-319-62415-0.S2CID1562290.^Pieter, Huang, Sandy Papernot, Nicolas Goodfellow, Ian Duan, Yan Abbeel (2017-02-07).Adversarial Attacks on Neural Network Policies.OCLC1106256905.{{cite book}}: CS1 maint: multiple names: authors list (link)^Korkmaz, Ezgi (2022)."Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs".Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22).36(7): 72297238.arXiv:2112.09025.doi:10.1609/aaai.v36i7.20684.S2CID245219157.^Berenji, H.R. (1994)."Fuzzy Q-learning: A new approach for fuzzy dynamic programming".Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference. Orlando, FL, USA: IEEE. pp. 486491.doi:10.1109/FUZZY.1994.343737.ISBN0-7803-1896-X.S2CID56694947.^Vincze, David (2017)."Fuzzy rule interpolation and reinforcement learning"(PDF).2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI). IEEE. pp. 173178.doi:10.1109/SAMI.2017.7880298.ISBN978-1-5090-5655-2.S2CID17590120.^Ng, A. Y.; Russell, S. J. (2000)."Algorithms for Inverse Reinforcement Learning"(PDF).Proceeding ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. pp. 663670.ISBN1-55860-707-2.^Ziebart, Brian D.; Maas, Andrew; Bagnell, J. Andrew; Dey, Anind K. (2008-07-13)."Maximum entropy inverse reinforcement learning".Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3. AAAI'08. Chicago, Illinois: AAAI Press: 14331438.ISBN978-1-57735-368-3.S2CID336219.^Pitombeira-Neto, Anselmo R.; Santos, Helano P.; Coelho da Silva, Ticiana L.; de Macedo, Jos Antonio F. (March 2024)."Trajectory modeling via random utility inverse reinforcement learning".Information Sciences.660: 120128.arXiv:2105.12092.doi:10.1016/j.ins.2024.120128.ISSN0020-0255.S2CID235187141.^Garca, Javier; Fernndez, Fernando (1 January 2015)."A comprehensive survey on safe reinforcement learning"(PDF).The Journal of Machine Learning Research.16(1): 14371480.^Dabney, Will; Ostrovski, Georg; Silver, David; Munos, Remi (2018-07-03)."Implicit Quantile Networks for Distributional Reinforcement Learning".Proceedings of the 35th International Conference on Machine Learning. PMLR: 10961105.arXiv:1806.06923.^Chow, Yinlam; Tamar, Aviv; Mannor, Shie; Pavone, Marco (2015)."Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach".Advances in Neural Information Processing Systems.28. Curran Associates, Inc.arXiv:1506.02188.^"Train Hard, Fight Easy: Robust Meta Reinforcement Learning".scholar.google.com. Retrieved2024-06-21.^Tamar, Aviv; Glassner, Yonatan; Mannor, Shie (2015-02-21)."Optimizing the CVaR via Sampling".Proceedings of the AAAI Conference on Artificial Intelligence.29(1).arXiv:1404.3862.doi:10.1609/aaai.v29i1.9561.ISSN2374-3468.^Greenberg, Ido; Chow, Yinlam; Ghavamzadeh, Mohammad; Mannor, Shie (2022-12-06)."Efficient Risk-Averse Reinforcement Learning".Advances in Neural Information Processing Systems.35: 3263932652.arXiv:2205.05138.^Bozinovski, S. (1982). "A self-learning system using secondary reinforcement". In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North-Holland. pp. 397402. ISBN 978-0-444-86488-8^Bozinovski S. (1995) "Neuro genetic agents and structural theory of self-reinforcement learning systems". CMPSCI Technical Report 95-107, University of Massachusetts at Amherst[1]^Bozinovski, S. (2014) "Modeling mechanisms of cognition-emotion interaction in artificial neural networks, since 1981." Procedia Computer Science p. 255-263^Engstrom, Logan; Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Janoos, Firdaus; Rudolph, Larry; Madry, Aleksander (2019-09-25)."Implementation Matters in Deep RL: A Case Study on PPO and TRPO".ICLR.^Colas, Cdric (2019-03-06)."A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms".In