=Pr(At=aSt=s){\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)}that maximizes the expected cumulative reward.Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to havefull observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to havepartial observability, and formally the problem must be formulated as apartially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion ofregret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, includingenergy storage,[6]robot control,[7]photovoltaic generators,[8]backgammon,checkers,[9]Go(AlphaGo), andautonomous driving systems.[10]Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use offunction approximationto deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:A model of the environment is known, but ananalytic solutionis not available;Only a simulation model of the environment is given (the subject ofsimulation-based optimization);[11]The only way to collect information about the environment is to interact with it.The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems tomachine learningproblems.Exploration[edit]The exploration vs. exploitation trade-off has been most thoroughly studied through themulti-armed banditproblem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.One such method is{\displaystyle \varepsilon }-greedy, where0<<1{\displaystyle 0<\varepsilon <1}is a parameter controlling the amount of exploration vs. exploitation. With probability1{\displaystyle 1-\varepsilon }, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability{\displaystyle \varepsilon }, exploration is chosen, and the action is chosen uniformly at random.{\displaystyle \varepsilon }is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]Algorithms for control learning[edit]Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.Criterion of optimality[edit]Policy[edit]The agent's action selection is modeled as a map calledpolicy::AS[0,1]{\displaystyle \pi :{\mathcal {A}}\times {\mathcal {S}}\rightarrow [0,1]}(a,s)=Pr(At=aSt=s){\displaystyle \pi (a,s)=\Pr(A_{t}=a\mid S_{t}=s)}The policy map gives the probability of taking actiona{\displaystyle a}when in states{\displaystyle s}.[14]: 61There are also deterministic policies.State-value function[edit]The state-value functionV(s){\displaystyle V_{\pi }(s)}is defined as,expected discounted returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60V(s)=E[GS0=s]=E[t=0tRt+1S0=s],{\displaystyle V_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards:G=t=0tRt+1=R1+R2+2R3+,{\disp