Weekly. Retrieved2021-06-17.^Williams, Rhiannon (2020-07-21)."Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'".i. Retrieved2021-06-17.^Kaplan, F.; Oudeyer, P. (2004). "Maximizing Learning Progress: An Internal Reward System for Development". In Iida, F.; Pfeifer, R.; Steels, L.; Kuniyoshi, Y. (eds.).Embodied Artificial Intelligence. Lecture Notes in Computer Science. Vol. 3139. Berlin; Heidelberg: Springer. pp. 259270.doi:10.1007/978-3-540-27833-7_19.ISBN978-3-540-22484-6.S2CID9781221.^Klyubin, A.; Polani, D.; Nehaniv, C. (2008)."Keep your options open: an information-based driving principle for sensorimotor systems".PLOS ONE.3(12): e4018.Bibcode:2008PLoSO...3.4018K.doi:10.1371/journal.pone.0004018.PMC2607028.PMID19107219.^Barto, A. G. (2013). "Intrinsic motivation and reinforcement learning".Intrinsically Motivated Learning in Natural and Artificial Systems(PDF). Berlin; Heidelberg: Springer. pp. 1747.^Dabrius, Kevin; Granat, Elvin; Karlsson, Patrik (2020). "Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks".The Journal of Machine Learning in Finance.1.SSRN3374766.^George Karimpanal, Thommen; Bouffanais, Roland (2019). "Self-organizing maps for storage and transfer of knowledge in reinforcement learning".Adaptive Behavior.27(2): 111126.arXiv:1811.08318.doi:10.1177/1059712318818568.ISSN1059-7123.S2CID53774629.^J Duan; Y Guan; S Li (2021)."Distributional Soft Actor-Critic: Off-policy reinforcement learning for addressing value estimation errors".IEEE Transactions on Neural Networks and Learning Systems.33(11): 65846598.arXiv:2001.02811.doi:10.1109/TNNLS.2021.3082568.PMID34101599.S2CID211259373.^Y Ren; J Duan; S Li (2020)."Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic".2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). pp. 16.arXiv:2002.05502.doi:10.1109/ITSC45102.2020.9294300.ISBN978-1-7281-4149-7.S2CID211096594.^Duan, J; Wang, W; Xiao, L (2023-10-26). "DSAC-T: Distributional Soft Actor-Critic with Three Refinements".arXiv:2310.05858[cs.LG].^Soucek, Branko (6 May 1992).Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series. John Wiley & Sons, Inc. p. 38.ISBN0-471-55717-X.^Francois-Lavet, Vincent; et al. (2018). "An Introduction to Deep Reinforcement Learning".Foundations and Trends in Machine Learning.11(34): 219354.arXiv:1811.12560.Bibcode:2018arXiv181112560F.doi:10.1561/2200000071.S2CID54434537.^Mnih, Volodymyr; et al. (2015). "Human-level control through deep reinforcement learning".Nature.518(7540): 529533.Bibcode:2015Natur.518..529M.doi:10.1038/nature14236.PMID25719670.S2CID205242740.^Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). "Explaining and Harnessing Adversarial Examples".International Conference on Learning Representations.arXiv:1412.6572.^Behzadan, Vahid; Munir, Arslan (2017). "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks".Machine Learning and Data Mining in Pattern Recognition. Lecture Notes in Computer Science. Vol. 10358. pp. 262275.arXiv:1701.04143.doi:10.1007/978-3-319-62416-7_19.ISBN978-3-319-62415-0.S2CID1562290.^Pieter, Huang, Sandy Papernot, Nicolas Goodfellow, Ian Duan, Yan Abbeel (2017-02-07).Adversarial Attacks on Neural Network Policies.OCLC1106256905.{{cite book}}: CS1 maint: multiple names: authors list (link)^Korkmaz, Ezgi (2022)."Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs".Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22).36(7): 72297238.arXiv:2112.09025.doi:10.1609/aaai.v36i7.20684.S2CID245219157.^Berenji, H.R. (1994)."Fuzzy Q-learning: A new approach for fuzzy dynamic programming".Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference. Orlando, FL, USA: IEEE. pp. 486491.doi:10.1109/FUZZY.1994.343737.ISBN0-7803-1896-X.S2CID56694947.^Vincze, David (2017)."Fuzzy rule interpolation and reinforcement learning"(PDF).2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI). IEEE. pp. 173178.doi:10.1109/SAMI.2017.7880298.ISBN978-1-5090-5655-2.S2CID17590120.^Ng, A. Y.; Russell, S. J. (2000)."Algorithms for Inverse Reinforcement Learning"(PDF).Proceeding ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. pp. 663670.ISBN1-55860-707-2.^Ziebart, Brian D.; Maas, Andrew; Bagnell, J. Andrew; Dey, Anind K. (2008-07-13)."Maximum entropy inverse reinforcement learning".Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3. AAAI'08. Chicago, Illinois: AAAI Press: 14331438.ISBN978-1-57735-368-3.S2CID336219.^Pitombeira-Neto, Anselmo R.; Santos, Helano P.; Coelho da Silva, Ticiana L.; de Macedo, Jos Antonio F. (March 2024)."Trajectory modeling via random utility inverse reinforcement learning".Information Sciences.660: 120128.arXiv:2105.12092.doi:10.1016/j.ins.2024.120128.ISSN0