Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.Direct policy search[edit]An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case ofstochastic optimization. The two approaches available are gradient-based and gradient-free methods.Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector{\displaystyle \theta }, let{\displaystyle \pi _{\theta }}denote the policy associated to{\displaystyle \theta }. Defining the performance function by()={\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }}}under mild conditions this function will be differentiable as a function of the parameter vector{\displaystyle \theta }. If the gradient of{\displaystyle \rho }was known, one could usegradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21](which is known as the likelihood ratio method in thesimulation-based optimizationliterature).[22]A large class of methods avoids relying on gradient information. These includesimulated annealing,cross-entropy searchor methods ofevolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years,actorcritic methodshave been proposed and performed well on various problems.[23]Policy search methods have been used in theroboticscontext.[24]Many policy search methods may get stuck in local optima (as they are based onlocal search).Model-based algorithms[edit]Finally, all of the above methods can be combined with algorithms that first learn a model of theMarkov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25]learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26]to the learning algorithm.Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]There are other ways to use models than to update a value function.[28]For instance, inmodel predictive controlthe model is used to update the behavior directly.Theory[edit]Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997).[12]Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).Research[edit]This sectionneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sourcesin this section. Unsourced material may be challenged and removed.(October 2022)(Learn how and when to remove this message)Research topics include:actor-critic architecture[29]actor-critic-scenery architecture[3]adaptive methods that work with fewer (or no) parameters under a large number of conditionsbug detection in software projects[30]continuous learningcombinations with logic-based frameworks[31]exploration in large Markov decision processeshuman feedback[32]interaction between implicit and explicit learning in skill acquisitionintrinsic motivationwhich differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluationslarge (or continuous) action spacesmodular and hierarchical reinforcemen