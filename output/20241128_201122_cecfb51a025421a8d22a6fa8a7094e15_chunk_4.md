_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards:G=t=0tRt+1=R1+R2+2R3+,{\displaystyle G=\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}=R_{1}+\gamma R_{2}+\gamma ^{2}R_{3}+\dots ,}whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount rate.{\displaystyle \gamma }is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-calledstationarypolicies. A policy isstationaryif the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted todeterministicstationary policies. Adeterministic stationarypolicy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.Brute force[edit]Thebrute forceapproach entails two steps:For each possible policy, sample returns while following itChoose the policy with the largest expected discounted returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this arevalue function estimationanddirect policy search.Value function[edit]See also:Value functionValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returnsE[G]{\displaystyle \operatorname {\mathbb {E} } [G]}for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return fromanyinitial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.To define optimality in a formal manner, define the state-value of a policy{\displaystyle \pi }byV(s)=E[Gs,],{\displaystyle V^{\pi }(s)=\operatorname {\mathbb {E} } [G\mid s,\pi ],}whereG{\displaystyle G}stands for the discounted return associated with following{\displaystyle \pi }from the initial states{\displaystyle s}. DefiningV(s){\displaystyle V^{*}(s)}as the maximum possible state-value ofV(s){\displaystyle V^{\pi }(s)}, where{\displaystyle \pi }is allowed to change,V(s)=maxV(s).{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}A policy that achieves these optimal state-values in each state is calledoptimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, sinceV(s)=maxE[Gs,]{\displaystyle V^{*}(s)=\max _{\pi }\mathbb {E} [G\mid s,\pi ]}, wheres{\displaystyle s}is a state randomly sampled from the distribution{\displaystyle \mu }of initial states (so(s)=Pr(S0=s){\displaystyle \mu (s)=\Pr(S_{0}=s)}).Although state-values suffice to define optimality, it is useful to define action-values. Given a states{\displaystyle s}, an actiona{\displaystyle a}and a policy{\displaystyle \pi }, the action-value of the pair(s,a){\displaystyle (s,a)}under{\displaystyle \pi }is defined byQ(s,a)=E[Gs,a,],{\displaystyle Q^{\pi }(s,a)=\operatorname {\mathbb {E} } [G\mid s,a,\pi ],\,}whereG{\displaystyle G}now stands for the random discounted return associated with first taking actiona{\displaystyle a}in states{\displaystyle s}and following{\displaystyle \pi }, thereafter.The theory of Markov decision processes states that if{\displaystyle \pi ^{*}}is an optimal policy, we act optimally (take the optimal action) by choosing the action fromQ(s,){\displaystyle Q^{\pi ^{*}}(s,\cdot )}with the highest action-value at each state,s{\displaystyle s}. Theaction-value functionof such an optimal policy (Q{\displaystyle Q^{\pi ^{*}}}) is called theoptimal action-value functionand is commonly denoted byQ{\displaystyle Q^{*}}. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.Assuming full knowledge of the Markov decision process, the two basic approaches to compute the opti