 3.3.1Monte Carlo methods 3.3.2Temporal difference methods 3.3.3Function approximation methods 3.4Direct policy search 3.5Model-based algorithms 4Theory 5Research 6Comparison of key algorithms 6.1Associative reinforcement learning 6.2Deep reinforcement learning 6.3Adversarial deep reinforcement learning 6.4Fuzzy reinforcement learning 6.5Inverse reinforcement learning 6.6Safe reinforcement learning 6.7Self-reinforcement learning 7Statistical comparison of reinforcement learning algorithms 8See also 9References 10Further reading 11External links Toggle the table of contentsReinforcement learning38 languagesBosanskiCataletinaDeutschEestiEspaolEuskaraFranaisBahasa IndonesiaItalianoBahasa MelayuNederlandsNorsk bokmlPolskiRuna SimiSimple EnglishSlovenina / srpskiSuomiSvenskaTrkeTing VitEdit linksArticleTalkEnglishReadEditView historyToolsToolsmove to sidebarhideActionsReadEditView historyGeneralWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codePrint/exportDownload as PDFPrintable versionIn other projectsWikimedia CommonsWikidata itemAppearancemove to sidebarhideFrom Wikipedia, the free encyclopediaField of machine learningFor reinforcement learning in psychology, seeReinforcementandOperant conditioning.Part of a series onMachine learninganddata miningParadigmsSupervised learningUnsupervised learningSemi-supervised learningSelf-supervised learningReinforcement learningMeta-learningOnline learningBatch learningCurriculum learningRule-based learningNeuro-symbolic AINeuromorphic engineeringQuantum machine learningProblemsClassificationGenerative modelingRegressionClusteringDimensionality reductionDensity estimationAnomaly detectionData cleaningAutoMLAssociation rulesSemantic analysisStructured predictionFeature engineeringFeature learningLearning to rankGrammar inductionOntology learningMultimodal learningSupervised learning(classificationregression)Apprenticeship learningDecision treesEnsemblesBaggingBoostingRandom forestk-NNLinear regressionNaive BayesArtificial neural networksLogistic regressionPerceptronRelevance vector machine (RVM)Support vector machine (SVM)ClusteringBIRCHCUREHierarchicalk-meansFuzzyExpectationmaximization (EM)DBSCANOPTICSMean shiftDimensionality reductionFactor analysisCCAICALDANMFPCAPGDt-SNESDLStructured predictionGraphical modelsBayes netConditional random fieldHidden MarkovAnomaly detectionRANSACk-NNLocal outlier factorIsolation forestArtificial neural networkAutoencoderDeep learningFeedforward neural networkRecurrent neural networkLSTMGRUESNreservoir computingBoltzmann machineRestrictedGANDiffusion modelSOMConvolutional neural networkU-NetLeNetAlexNetDeepDreamNeural radiance fieldTransformerVisionMambaSpiking neural networkMemtransistorElectrochemical RAM(ECRAM)Reinforcement learningQ-learningSARSATemporal difference (TD)Multi-agentSelf-playLearning with humansActive learningCrowdsourcingHuman-in-the-loopRLHFModel diagnosticsCoefficient of determinationConfusion matrixLearning curveROC curveMathematical foundationsKernel machinesBiasvariance tradeoffComputational learning theoryEmpirical risk minimizationOccam learningPAC learningStatistical learningVC theoryJournals and conferencesECML PKDDNeurIPSICMLICLRIJCAIMLJMLRRelated articlesGlossary of artificial intelligenceList of datasets for machine-learning researchList of datasets in computer vision and image processingOutline of machine learningvteReinforcement learning(RL) is an interdisciplinary area ofmachine learningandoptimal controlconcerned with how anintelligent agentshouldtake actionsin a dynamic environment in order tomaximize a rewardsignal. Reinforcement learning is one of thethree basic machine learning paradigms, alongsidesupervised learningandunsupervised learning.Q-learningat its simplest stores data in tables. This approach becomesinfeasibleas the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes.Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).[1]The search for this balance is known as theexploration-exploitation dilemma.The environment is typically stated in the form of aMarkov decision process(MDP), as many reinforcement learning algorithms usedynamic programmingtechniques.[2]The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods beco