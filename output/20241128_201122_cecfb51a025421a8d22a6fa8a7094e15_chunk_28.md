st one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems tomachine learningproblems.Exploration[edit]The exploration vs. exploitation trade-off has been most thoroughly studied through themulti-armed banditproblem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.One such method is{\displaystyle \varepsilon }-greedy, where0<<1{\displaystyle 0<\varepsilon <1}is a parameter controlling the amount of exploration vs. exploitation. With probability1{\displaystyle 1-\varepsilon }, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability{\displaystyle \varepsilon }, exploration is chosen, and the action is chosen uniformly at random.{\displaystyle \varepsilon }is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]Algorithms for control learning[edit]Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.Criterion of optimality[edit]Policy[edit]The agent's action selection is modeled as a map calledpolicy::AS[0,1]{\displaystyle \pi :{\mathcal {A}}\times {\mathcal {S}}\rightarrow [0,1]}(a,s)=Pr(At=aSt=s){\displaystyle \pi (a,s)=\Pr(A_{t}=a\mid S_{t}=s)}The policy map gives the probability of taking actiona{\displaystyle a}when in states{\displaystyle s}.[14]: 61There are also deterministic policies.State-value function[edit]The state-value functionV(s){\displaystyle V_{\pi }(s)}is defined as,expected discounted returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60V(s)=E[GS0=s]=E[t=0tRt+1S0=s],{\displaystyle V_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards:G=t=0tRt+1=R1+R2+2R3+,{\displaystyle G=\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}=R_{1}+\gamma R_{2}+\gamma ^{2}R_{3}+\dots ,}whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount rate.{\displaystyle \gamma }is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-calledstationarypolicies. A policy isstationaryif the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted todeterministicstationary policies. Adeterministic stationarypolicy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.Brute force[edit]Thebrute forceapproach entails two steps:For each possible policy, sample returns while following itChoose the policy with the largest expected discounted returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this arevalue function estimationanddirect policy search.Value function[edit]See also:Value functionValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returnsE[G]{\displaystyle \operatorname {\mathbb {E} } [G]}for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).These methods rely 