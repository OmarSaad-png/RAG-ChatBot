In the operations research and control literature, RL is calledapproximate dynamic programming, orneuro-dynamic programming.The problems of interest in RL have also been studied in thetheory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).Basic reinforcement learning is modeled as aMarkov decision process:A set of environment and agent states (the state space),S{\displaystyle {\mathcal {S}}};A set of actions (the action space),A{\displaystyle {\mathcal {A}}}, of the agent;Pa(s,s)=Pr(St+1=sSt=s,At=a){\displaystyle P_{a}(s,s')=\Pr(S_{t+1}=s'\mid S_{t}=s,A_{t}=a)}, the transition probability (at timet{\displaystyle t}) from states{\displaystyle s}to states{\displaystyle s'}under actiona{\displaystyle a}.Ra(s,s){\displaystyle R_{a}(s,s')}, the immediate reward after transition froms{\displaystyle s}tos{\displaystyle s'}under actiona{\displaystyle a}.The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar toprocessesthat appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret