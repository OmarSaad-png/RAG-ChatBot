ent years,actorcritic methodshave been proposed and performed well on various problems.[23]Policy search methods have been used in theroboticscontext.[24]Many policy search methods may get stuck in local optima (as they are based onlocal search).Model-based algorithms[edit]Finally, all of the above methods can be combined with algorithms that first learn a model of theMarkov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25]learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26]to the learning algorithm.Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]There are other ways to use models than to update a value function.[28]For instance, inmodel predictive controlthe model is used to update the behavior directly.Theory[edit]Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997).[12]Finite-time performance bounds have also