thesubstantia nigrato thebasal gangliafunction are the prediction error.value-function and policy search methodsComparison of key algorithms[edit]AlgorithmDescriptionPolicyAction spaceState spaceOperatorMonte CarloEvery visit to Monte CarloEitherDiscreteDiscreteSample-means of state-values or action-valuesTD learningStateactionrewardstateOff-policyDiscreteDiscreteState-valueQ-learningStateactionrewardstateOff-policyDiscreteDiscreteAction-valueSARSAStateactionrewardstateactionOn-policyDiscreteDiscreteAction-valueDQNDeep Q NetworkOff-policyDiscreteContinuousAction-valueDDPGDeep Deterministic Policy GradientOff-policyContinuousContinuousAction-valueA3CAsynchronous Advantage Actor-Critic AlgorithmOn-policyDiscreteContinuousAdvantage (=action-value - state-value)TRPOTrust Region Policy OptimizationOn-policyContinuous or DiscreteContinuousAdvantagePPOProximal Policy OptimizationOn-policyContinuous or DiscreteContinuousAdvantageTD3Twin Delayed Deep Deterministic Policy GradientOff-policyContinuousContinuousAction-valueSACSoft Actor-CriticOff-policyContinuousContinuousAdvantageDSAC[43][44][45]Distributional Soft Actor CriticOff-policyContinuousContinuousAction-value distributionAssociative reinforcement learning[edit]Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]Deep