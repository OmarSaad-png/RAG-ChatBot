the transition probability (at timet{\displaystyle t}) from states{\displaystyle s}to states{\displaystyle s'}under actiona{\displaystyle a}.Ra(s,s){\displaystyle R_{a}(s,s')}, the immediate reward after transition froms{\displaystyle s}tos{\displaystyle s'}under actiona{\displaystyle a}.The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar toprocessesthat appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time stept, the agent receives the current stateSt{\displaystyle S_{t}}and rewardRt{\displaystyle R_{t}}. It then chooses an actionAt{\displaystyle A_{t}}from the set of available actions, which is subsequently sent to the environment. The environment moves to a new stateSt+1{\displaystyle S_{t+1}}and the rewardRt+1{\displaystyle R_{t+1}}associated with thetransition(St,At,St+1){\displaystyle (S_{t},A_{t},S_{t+1})}is determined. The goal of a reinforcement learning agent is