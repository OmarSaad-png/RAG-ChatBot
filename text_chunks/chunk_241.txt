ecision processes it is known that, without loss of generality, the search can be restricted to the set of so-calledstationarypolicies. A policy isstationaryif the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted todeterministicstationary policies. Adeterministic stationarypolicy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.Brute force[edit]Thebrute forceapproach entails two steps:For each possible policy, sample returns while following itChoose the policy with the largest expected discounted returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this arevalue function estimationanddirect policy search.Value function[edit]See also:Value functionValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returnsE[G]{\displaystyle