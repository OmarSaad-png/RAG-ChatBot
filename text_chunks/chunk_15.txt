that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency. An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods. Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector  {\displaystyle \theta } , let   {\displaystyle \pi _{\theta }} denote the policy associated to  {\displaystyle \theta } . Defining the performance function by  (  ) =    {\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }}} under mild conditions this function will be differentiable as a function of the parameter vector  {\displaystyle \theta } . If the gradient of  {\displaystyle \rho } was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22] A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search