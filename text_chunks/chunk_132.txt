S_{t+1}}and the rewardRt+1{\displaystyle R_{t+1}}associated with thetransition(St,At,St+1){\displaystyle (S_{t},A_{t},S_{t+1})}is determined. The goal of a reinforcement learning agent is to learn apolicy::SA[0,1]{\displaystyle \pi :{\mathcal {S}}\times {\mathcal {A}}\rightarrow [0,1]},(s,a)=Pr(At=aSt=s){\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)}that maximizes the expected cumulative reward.Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to havefull observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to havepartial observability, and formally the problem must be formulated as apartially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, th