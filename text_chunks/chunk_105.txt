toring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[67]After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to bei.i.d, standard statistical tools can be used for hypothesis testing, such asT-testandpermutation test.[68]This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[69]See also[edit]Temporal difference learningQ-learningStateactionrewardstateaction(SARSA)Reinforcement learning from human feedbackOptimal controlError-driven learningMulti-agent reinforcement learningApprenticeship learningModel-free (reinforcement learning)active learning (machine learning)References[edit]^Kaelbling, Leslie P.;Littman, Michael L.;Moore, Andrew W.(1996)."Reinforcement Learning: A Survey".Journal of Artificial Intelligence Research.4: 237285.arXiv:cs/9605103.doi:10.1613/jair.301.S2CID1708582. Archived fromthe originalon