gradient-free methods can achieve (in theory and in the limit) a global optimum. Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years,actorcritic methodshave been proposed and performed well on various problems.[23] Policy search methods have been used in theroboticscontext.[24]Many policy search methods may get stuck in local optima (as they are based onlocal search). Model-based algorithms[edit] Finally, all of the above methods can be combined with algorithms that first learn a model of theMarkov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25]learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transi