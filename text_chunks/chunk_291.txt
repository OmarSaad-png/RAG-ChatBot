discounted rewards: whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount rate.{\displaystyle \gamma }is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future. The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-calledstationarypolicies. A policy isstationaryif the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted todeterministicstationary policies. Adeterministic stationarypolicy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality. Brute force[edit] Thebrute forceapproach entails two steps: One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy. These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others.