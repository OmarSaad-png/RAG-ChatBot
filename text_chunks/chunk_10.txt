\pi } , the action-value of the pair ( s , a ) {\displaystyle (s,a)} under  {\displaystyle \pi } is defined by where G {\displaystyle G} now stands for the random discounted return associated with first taking action a {\displaystyle a} in state s {\displaystyle s} and following  {\displaystyle \pi } , thereafter. The theory of Markov decision processes states that if   {\displaystyle \pi ^{*}} is an optimal policy, we act optimally (take the optimal action) by choosing the action from Q   ( s ,  ) {\displaystyle Q^{\pi ^{*}}(s,\cdot )} with the highest action-value at each state, s {\displaystyle s} . The action-value function of such an optimal policy ( Q   {\displaystyle Q^{\pi ^{*}}} ) is called the optimal action-value function and is commonly denoted by Q  {\displaystyle Q^{*}} . In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally. Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions Q k {\displaystyle Q_{k}} ( k = 0 , 1 , 2 ,  {\displaystyle k=0,1,2,\ldots } ) that converge to Q  {\displaystyle Q^{*}} . Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over