laystyle G=\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}=R_{1}+\gamma R_{2}+\gamma ^{2}R_{3}+\dots ,}whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount rate.{\displaystyle \gamma }is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-calledstationarypolicies. A policy isstationaryif the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted todeterministicstationary policies. Adeterministic stationarypolicy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.Brute force[edit]Thebrute forceapproach entails two steps:For each possible policy, sample returns while following itChoose the policy with the largest expected discounted returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate