the Markov Decision Process can be learnt.[27]There are other ways to use models than to update a value function.[28]For instance, inmodel predictive controlthe model is used to update the behavior directly.Theory[edit]Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997).[12]Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).Research[edit]This sectionneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sourcesin this section. Unsourced material may be challenged and removed.(October 2022)(Learn how and when to remove this message)Research topics include:actor-critic architecture[29]actor-critic-scenery architecture[3]adaptive methods that work with fewer (or no) parameters under a large number of conditionsbug detection in software projects[30]continuous learningcombinations