g, where instead of theexpectedreturn, arisk-measureof the return is optimized, such as theConditional Value at Risk(CVaR).[59]In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61]However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62]and blindness to success.[63] Self-reinforcement learning[edit] Self-reinforcement learning (or self learning), is a learning paradigm which does not use the concept of immediate reward Ra(s,s') after transition from s to s' with action a. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation. The self-reinforcement algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 1. in situation s perform action a 2. receive a consequence situation s' 3. compute state evaluation v(s') of how good is to be in the consequence situation s' 4. update crossbar memory w'(a,s) = w(a,s) + v(s') Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior). Self reinforcement (self