cumulative rewards.Criterion of optimality[edit]Policy[edit]The agent's action selection is modeled as a map calledpolicy::AS[0,1]{\displaystyle \pi :{\mathcal {A}}\times {\mathcal {S}}\rightarrow [0,1]}(a,s)=Pr(At=aSt=s){\displaystyle \pi (a,s)=\Pr(A_{t}=a\mid S_{t}=s)}The policy map gives the probability of taking actiona{\displaystyle a}when in states{\displaystyle s}.[14]: 61There are also deterministic policies.State-value function[edit]The state-value functionV(s){\displaystyle V_{\pi }(s)}is defined as,expected discounted returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60V(s)=E[GS0=s]=E[t=0tRt+1S0=s],{\displaystyle V_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards:G=t=0tRt+1=R1+R2+2R3+,{\disp