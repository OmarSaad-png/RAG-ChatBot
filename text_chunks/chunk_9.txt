ely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies. To define optimality in a formal manner, define the state-value of a policy  {\displaystyle \pi } by where G {\displaystyle G} stands for the discounted return associated with following  {\displaystyle \pi } from the initial state s {\displaystyle s} . Defining V  ( s ) {\displaystyle V^{*}(s)} as the maximum possible state-value of V  ( s ) {\displaystyle V^{\pi }(s)} , where  {\displaystyle \pi } is allowed to change, A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since V  ( s ) = max  E [ G  s ,  ] {\displaystyle V^{*}(s)=\max _{\pi }\mathbb {E} [G\mid s,\pi ]} , where s {\displaystyle s} is a state randomly sampled from the distribution  {\displaystyle \mu } of initial states (so  ( s ) = Pr ( S 0 = s ) {\displaystyle \mu (s)=\Pr(S_{0}=s)} ). Although state-values suffice to define optimality, it is useful to define action-values. Given a state s {\displaystyle s} , an action a {\displaystyle a} and a policy  {\displaystyle \pi } , the action-value of the pair ( s , a )