Y Guan; S Li (2021)."Distributional Soft Actor-Critic: Off-policy reinforcement learning for addressing value estimation errors".IEEE Transactions on Neural Networks and Learning Systems.33(11): 65846598.arXiv:2001.02811.doi:10.1109/TNNLS.2021.3082568.PMID34101599.S2CID211259373.^Y Ren; J Duan; S Li (2020)."Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic".2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). pp. 16.arXiv:2002.05502.doi:10.1109/ITSC45102.2020.9294300.ISBN978-1-7281-4149-7.S2CID211096594.^Duan, J; Wang, W; Xiao, L (2023-10-26). "DSAC-T: Distributional Soft Actor-Critic with Three Refinements".arXiv:2310.05858[cs.LG].^Soucek, Branko (6 May 1992).Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series. John Wiley & Sons, Inc. p. 38.ISBN0-471-55717-X.^Francois-Lavet, Vincent; et al. (2018). "An Introduction to Deep Reinforcement Learning".Foundations and Trends in Machine Learning.11(34): 219354.arXiv:1811.12560.Bibcode:2018arXiv181112560F.doi:10.1561/2200000071.S2CID54434537.^Mnih, Volodymyr; et al. (2015). "Human-level control through deep reinforcement learning".Nature.518(7540): 529533.Bibcode:2015Natur.518..529M.doi:10.1038/nature14236.PMID25719670.S2CID205242740.^Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). "Explaining and Harnessing Adversarial Examples".International Conference on Learning