Alternatively, with probability{\displaystyle \varepsilon }, exploration is chosen, and the action is chosen uniformly at random.{\displaystyle \varepsilon }is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13] Algorithms for control learning[edit] Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards. Criterion of optimality[edit] Policy[edit] The agent's action selection is modeled as a map calledpolicy: The policy map gives the probability of taking actiona{\displaystyle a}when in states{\displaystyle s}.[14]: 61There are also deterministic policies. State-value function[edit] The state-value functionV(s){\displaystyle V_{\pi }(s)}is defined as,expected discounted returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60 where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards: whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount