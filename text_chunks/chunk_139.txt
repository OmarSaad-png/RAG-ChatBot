}, exploration is chosen, and the action is chosen uniformly at random.{\displaystyle \varepsilon }is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]Algorithms for control learning[edit]Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.Criterion of optimality[edit]Policy[edit]The agent's action selection is modeled as a map calledpolicy::AS[0,1]{\displaystyle \pi :{\mathcal {A}}\times {\mathcal {S}}\rightarrow [0,1]}(a,s)=Pr(At=aSt=s){\displaystyle \pi (a,s)=\Pr(A_{t}=a\mid S_{t}=s)}The policy map gives the probability of taking actiona{\displaystyle a}when in states{\displaystyle s}.[14]: 61There are also deterministic policies.State-value function[edit]The state-value functionV(s){\displaystyle V_{\pi }(s)}is defined as,expected discounted returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60V(s)=E[GS0=s]=E[t=0tRt+1S0=s],{\displaystyle V_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle