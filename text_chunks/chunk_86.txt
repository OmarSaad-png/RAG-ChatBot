\mu (s)=\Pr(S_{0}=s)}).Although state-values suffice to define optimality, it is useful to define action-values. Given a states{\displaystyle s}, an actiona{\displaystyle a}and a policy{\displaystyle \pi }, the action-value of the pair(s,a){\displaystyle (s,a)}under{\displaystyle \pi }is defined byQ(s,a)=E[Gs,a,],{\displaystyle Q^{\pi }(s,a)=\operatorname {\mathbb {E} } [G\mid s,a,\pi ],\,}whereG{\displaystyle G}now stands for the random discounted return associated with first taking actiona{\displaystyle a}in states{\displaystyle s}and following{\displaystyle \pi }, thereafter.The theory of Markov decision processes states that if{\displaystyle \pi ^{*}}is an optimal policy, we act optimally (take the optimal action) by choosing the action fromQ(s,){\displaystyle Q^{\pi ^{*}}(s,\cdot )}with the highest action-value at each state,s{\displaystyle s}. Theaction-value functionof such an optimal policy (Q{\displaystyle Q^{\pi ^{*}}}) is called theoptimal action-value functionand is commonly denoted byQ{\displaystyle Q^{*}}. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function arevalue iterationandpolicy iteration. Both algorithms compute a sequence of functionsQk{\displaystyle Q_{k}}(k=0,1,2,{\displaystyle k=0,1,2,\ldots }) that converge toQ{\displaystyle Q^{*}}. Computing these functions involves