style \pi }is allowed to change,V(s)=maxV(s).{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}A policy that achieves these optimal state-values in each state is calledoptimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, sinceV(s)=maxE[Gs,]{\displaystyle V^{*}(s)=\max _{\pi }\mathbb {E} [G\mid s,\pi ]}, wheres{\displaystyle s}is a state randomly sampled from the distribution{\displaystyle \mu }of initial states (so(s)=Pr(S0=s){\displaystyle \mu (s)=\Pr(S_{0}=s)}).Although state-values suffice to define optimality, it is useful to define action-values. Given a states{\displaystyle s}, an actiona{\displaystyle a}and a policy{\displaystyle \pi }, the action-value of the pair(s,a){\displaystyle (s,a)}under{\displaystyle \pi }is defined byQ(s,a)=E[Gs,a,],{\displaystyle Q^{\pi }(s,a)=\operatorname {\mathbb {E} } [G\mid s,a,\pi ],\,}whereG{\displaystyle G}now stands for the random discounted return associated with first taking actiona{\displaystyle a}in states{\displaystyle s}and following{\displaystyle \pi }, thereafter.The theory of Markov decision processes states that if{\displaystyle \pi ^{*}}is an optimal policy, we act optimally (take the optimal action) by choosing the action fromQ(s,){\displaystyle Q^{\pi ^{*}}(s,\cdot )}with the highest action-value at each state,s{\displaystyle s}. Theaction-value functionof such an optimal policy (Q{\displaystyle Q^{\pi ^{*}}}) is called theoptimal