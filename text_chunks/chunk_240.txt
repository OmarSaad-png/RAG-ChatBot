returnstarting with states{\displaystyle s}, i.e.S0=s{\displaystyle S_{0}=s}, and successively following policy{\displaystyle \pi }. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[14]: 60V(s)=E[GS0=s]=E[t=0tRt+1S0=s],{\displaystyle V_{\pi }(s)=\operatorname {\mathbb {E} } [G\mid S_{0}=s]=\operatorname {\mathbb {E} } \left[\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}\mid S_{0}=s\right],}where the random variableG{\displaystyle G}denotes thediscounted return, and is defined as the sum of future discounted rewards:G=t=0tRt+1=R1+R2+2R3+,{\displaystyle G=\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}=R_{1}+\gamma R_{2}+\gamma ^{2}R_{3}+\dots ,}whereRt+1{\displaystyle R_{t+1}}is the reward for transitioning from stateSt{\displaystyle S_{t}}toSt+1{\displaystyle S_{t+1}},0<1{\displaystyle 0\leq \gamma <1}is thediscount rate.{\displaystyle \gamma }is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.The algorithm must find a policy with maximum expected discounted return. From the theory of Markov d