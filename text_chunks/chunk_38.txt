ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21](which is known as the likelihood ratio method in thesimulation-based optimizationliterature).[22]A large class of methods avoids relying on gradient information. These includesimulated annealing,cross-entropy searchor methods ofevolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years,actorcritic methodshave been proposed and performed well on various problems.[23]Policy search methods have been used in theroboticscontext.[24]Many policy search methods may get stuck in local optima (as they are based onlocal search).Model-based algorithms[edit]Finally, all of the above methods can be combined with algorithms that first learn a model of theMarkov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25]learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such