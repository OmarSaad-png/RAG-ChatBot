function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returnsE[G]{\displaystyle \operatorname {\mathbb {E} } [G]}for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return fromanyinitial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.To define optimality in a formal manner, define the state-value of a policy{\displaystyle \pi }byV(s)=E[Gs,],{\displaystyle V^{\pi }(s)=\operatorname {\mathbb {E} } [G\mid s,\pi ],}whereG{\displaystyle G}stands for the discounted return associated with following{\displaystyle \pi }from the initial states{\displaystyle s}. DefiningV(s){\displaystyle V^{*}(s)}as the maximum possible state-value ofV(s){\displaystyle V^{\pi }(s)}, where{\display