interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.Function approximation methods[edit]In order to address the fifth issue,function approximation methodsare used.Linear function approximationstarts with a mapping{\displaystyle \phi }that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair(s,a){\displaystyle (s,a)}are obtained by linearly combining the components of(s,a){\displaystyle \phi (s,a)}with someweights{\displaystyle \theta }:Q(s,a)=i=1dii(s,a).{\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas fromnonparametric statistics(which can be seen to construct their own features) have been explored.Value iteration can also be used as a starting point, giving rise to theQ-learningalgorithm and its many variants.[19]