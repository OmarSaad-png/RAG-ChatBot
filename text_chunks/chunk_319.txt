(ITSC). pp. 16.arXiv:2002.05502.doi:10.1109/ITSC45102.2020.9294300.ISBN978-1-7281-4149-7.S2CID211096594.^Duan, J; Wang, W; Xiao, L (2023-10-26). "DSAC-T: Distributional Soft Actor-Critic with Three Refinements".arXiv:2310.05858[cs.LG].^Soucek, Branko (6 May 1992).Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series. John Wiley & Sons, Inc. p. 38.ISBN0-471-55717-X.^Francois-Lavet, Vincent; et al. (2018). "An Introduction to Deep Reinforcement Learning".Foundations and Trends in Machine Learning.11(34): 219354.arXiv:1811.12560.Bibcode:2018arXiv181112560F.doi:10.1561/2200000071.S2CID54434537.^Mnih, Volodymyr; et al. (2015). "Human-level control through deep reinforcement learning".Nature.518(7540): 529533.Bibcode:2015Natur.518..529M.doi:10.1038/nature14236.PMID25719670.S2CID205242740.^Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). "Explaining and Harnessing Adversarial Examples".International Conference on Learning Representations.arXiv:1412.6572.^Behzadan, Vahid; Munir, Arslan (2017). "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks".Machine Learning and Data Mining in Pattern Recognition. Lecture Notes in Computer Science. Vol. 10358. pp. 262275.arXiv:1701.04143.doi:10.1007/978-3-319-62416-7_19.ISBN978-3-319-62415-0.S2CID1562290.^Pieter, Huang, Sandy Papernot, Nicolas Goodfellow, Ian Duan, Yan Abbeel (2017-02-07).Adversarial Attacks on Neural Network Policies.OCLC1106256905.{{cite book}}: CS1 maint: