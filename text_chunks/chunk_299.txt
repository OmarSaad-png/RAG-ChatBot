Direct policy search[edit] An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case ofstochastic optimization. The two approaches available are gradient-based and gradient-free methods. Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector{\displaystyle \theta }, let{\displaystyle \pi _{\theta }}denote the policy associated to{\displaystyle \theta }. Defining the performance function by()={\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }}}under mild conditions this function will be differentiable as a function of the parameter vector{\displaystyle \theta }. If the gradient of{\displaystyle \rho }was known, one could usegradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21](which is known as the likelihood ratio method in thesimulation-based optimizationliterature).[22] A large class of methods avoids relying on gradient information. These includesimulated annealing,cross-entropy searchor methods ofevolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum. Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the