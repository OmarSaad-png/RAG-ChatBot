practical. One such method is  {\displaystyle \varepsilon } -greedy, where 0 <  < 1 {\displaystyle 0<\varepsilon <1} is a parameter controlling the amount of exploration vs. exploitation. With probability 1   {\displaystyle 1-\varepsilon } , exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability  {\displaystyle \varepsilon } , exploration is chosen, and the action is chosen uniformly at random.  {\displaystyle \varepsilon } is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13] Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards. The agent's action selection is modeled as a map called policy: The policy map gives the probability of taking action a {\displaystyle a} when in state s {\displaystyle s} .[14]: 61 There are also deterministic policies. The state-value function V  ( s ) {\displaystyle V_{\pi }(s)} is defined as, expected discounted return starting with state s {\displaystyle s} , i.e. S 0 = s {\displaystyle S_{0}=s} , and successively following policy  {\displaystyle \pi } . Hence, roughly speaking, the value function estimates "how good" it is to be in a given