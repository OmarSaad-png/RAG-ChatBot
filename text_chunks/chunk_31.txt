V^{\pi }(s)=\operatorname {\mathbb {E} } [G\mid s,\pi ],}whereG{\displaystyle G}stands for the discounted return associated with following{\displaystyle \pi }from the initial states{\displaystyle s}. DefiningV(s){\displaystyle V^{*}(s)}as the maximum possible state-value ofV(s){\displaystyle V^{\pi }(s)}, where{\displaystyle \pi }is allowed to change,V(s)=maxV(s).{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}A policy that achieves these optimal state-values in each state is calledoptimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, sinceV(s)=maxE[Gs,]{\displaystyle V^{*}(s)=\max _{\pi }\mathbb {E} [G\mid s,\pi ]}, wheres{\displaystyle s}is a state randomly sampled from the distribution{\displaystyle \mu }of initial states (so(s)=Pr(S0=s){\displaystyle \mu (s)=\Pr(S_{0}=s)}).Although state-values suffice to define optimality, it is useful to define action-values. Given a states{\displaystyle s}, an actiona{\displaystyle a}and a policy{\displaystyle \pi }, the action-value of the pair(s,a){\displaystyle (s,a)}under{\displaystyle \pi }is defined byQ(s,a)=E[Gs,a,],{\displaystyle Q^{\pi }(s,a)=\operatorname {\mathbb {E} } [G\mid s,a,\pi ],\,}whereG{\displaystyle G}now stands for the random discounted return associated with first taking actiona{\displaystyle a}in states{\displaystyle s}and following{\displaystyle \pi }, thereafter.The theory of Markov decision processes states that