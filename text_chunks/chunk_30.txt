returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this arevalue function estimationanddirect policy search.Value function[edit]See also:Value functionValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returnsE[G]{\displaystyle \operatorname {\mathbb {E} } [G]}for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return fromanyinitial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.To define optimality in a formal manner, define the state-value of a policy{\displaystyle \pi }byV(s)=E[Gs,],{\displaystyle V^{\pi }(s)=\operatorname {\mathbb {E} } [G\mid s,\pi ],}whereG{\displaystyle G}stands for the discounted return associated with following{\displaystyle \pi }from the initial states{\displaystyle