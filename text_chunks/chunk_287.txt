discrete time steps. At each time stept, the agent receives the current stateSt{\displaystyle S_{t}}and rewardRt{\displaystyle R_{t}}. It then chooses an actionAt{\displaystyle A_{t}}from the set of available actions, which is subsequently sent to the environment. The environment moves to a new stateSt+1{\displaystyle S_{t+1}}and the rewardRt+1{\displaystyle R_{t+1}}associated with thetransition(St,At,St+1){\displaystyle (S_{t},A_{t},S_{t+1})}is determined. The goal of a reinforcement learning agent is to learn apolicy: :SA[0,1]{\displaystyle \pi :{\mathcal {S}}\times {\mathcal {A}}\rightarrow [0,1]},(s,a)=Pr(At=aSt=s){\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)} that maximizes the expected cumulative reward. Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to havefull observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to havepartial observability, and formally the problem must be formulated as apartially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed. When the agent's performance is compared to that of an